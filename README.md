```markdown
# ğŸš€ End-to-End Production Grade MLOps Project  

This repository demonstrates how to take a **simple ML problem (text classification)** and turn it into a **fully automated, production-grade MLOps pipeline**.  
The goal was not just accuracy â€” but to design the **engineering backbone** required for deploying and monitoring ML models at scale.  

---

## ğŸ“Œ Project Overview  

- **Problem Statement:** Text Classification (predicting labels from text data)  
- **Objective:** Build an end-to-end ML system with data management, experiment tracking, automation, deployment, and monitoring  
- **Tech Stack:**  
  - **Cloud & Infra:** AWS S3, AWS ECR, AWS EKS  
  - **Experiment Tracking:** Dagshub + MLflow  
  - **Pipeline Orchestration:** DVC  
  - **CI/CD:** GitHub Actions  
  - **Containerization & Deployment:** Docker, Kubernetes  
  - **Monitoring:** Prometheus + Grafana  

---

## âš™ï¸ Pipeline Architecture  

The project is structured as modular pipelines under the `src/` folder:  

```

src/
â”‚â”€â”€ data\_ingestion.py
â”‚â”€â”€ data\_preprocessing.py
â”‚â”€â”€ feature\_engineering.py
â”‚â”€â”€ model\_building.py
â”‚â”€â”€ model\_evaluation.py
â”‚â”€â”€ register\_model.py

````

Each stage feeds into the next, creating a reproducible workflow.  

- **Data Ingestion:** Loads raw data from AWS S3  
- **Preprocessing & Feature Engineering:** Cleans and transforms data  
- **Model Training:** Trains ML models with tracked hyperparameters  
- **Evaluation:** Compares metrics across runs  
- **Model Registry:** Implements **Challenger vs. Champion strategy** â€” only better models replace the production one  

---

## ğŸ”„ Workflow Automation  

- **DVC Pipeline:**  
  Defined in `dvc.yaml` for reproducible pipelines. Run the full pipeline with:  
  ```bash
  dvc repro
````

* **CI/CD with GitHub Actions:**
  Ensures code quality and pipeline integrity with automated testing (`.github/workflows/ci.yaml`).

---

## ğŸ“¦ Deployment

* **Dockerized the pipeline** and pushed images to **AWS ECR**
* Deployed on **Kubernetes (EKS)** for scalability and reliability

---

## ğŸ“Š Monitoring

* Integrated **Prometheus + Grafana** to monitor:

  * Model performance metrics
  * System health (CPU, memory, latency)

---

## ğŸ“¸ Screenshots

* MLflow Dashboard (experiment runs & metrics comparison)
* DVC DAG (pipeline graph)
* GitHub Actions workflow passing âœ…
* Grafana dashboard with live metrics

*(Screenshots are available in the `assets/` folder)*

---

## ğŸ› ï¸ How to Run Locally

1. Clone the repo:

   ```bash
   git clone https://github.com/<username>/<repo>.git
   cd <repo>
   ```

2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Run DVC pipeline:

   ```bash
   dvc repro
   ```

4. Launch MLflow UI for experiment tracking:

   ```bash
   mlflow ui
   ```

---

## ğŸš€ Key Learnings

* How to move from **notebooks â†’ production pipelines**
* Setting up **experiment tracking, model registry, and versioning**
* Designing **CI/CD for ML workflows**
* Deploying on **cloud-native infrastructure (AWS + Kubernetes)**
* Monitoring with **Prometheus & Grafana**

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ src/                 # Modular pipeline scripts
â”œâ”€â”€ dvc.yaml             # DVC pipeline definition
â”œâ”€â”€ .github/workflows/   # CI/CD workflows
â”œâ”€â”€ Dockerfile           # Containerization
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md            # Project documentation
â””â”€â”€ assets/              # Screenshots & visuals
```

---

## ğŸ“¬ Contact

ğŸ‘¤ Syed Abdul Rehman

* LinkedIn: linkedin.com/in/sa-rehman1/
* Email: contactsyed135@gmail.com

---

âœ¨ *This project showcases how to productionize ML workflows, making them scalable, reproducible, and deployment-ready.*

```

